# 基本流程
决策树是基于树结构来进行决策的。一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的路径对应了一个判定测试序列。决策树的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单而直观的“分而治之”策略
决策树的生成是一个递归过程。

输入：训练集D={(x1,y1),(x2,y2),……，(xm,ym)};
    属性集A={a1,a2,……,ad}
过程:函数TreeGenerate(D,A)
1. 生成节点node;
2.  if D中样本全属于同一类别C then
     将node标记为C类结点； return 
    end if
3.  if A=$\emptyset$ OR D中样本在A上取值相同 then 
      将node标记为叶结点，其类别标记为D中样本数最多的类；return
    end if
4.  从A中选择最优划分属性a*
    for a* 的每个值$a*^v$do
     为node生成一个分支；令$D_v$表示D中在a*上取值为$a*^v$的样本子集
     if $D_v$ 为空then
      将分支结点标记为叶结点，其类别标记为D中样本最多的类；return
     else
      以TreeGenerate($D_v,A\{a*}$)为分支结点
    end if
    end for

在决策树基本算法中，有三种情形会导致递归返回：
（1）当前结点包含的样本全属于同一类别，无需划分
（2）当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
（3）当前结点包含的样本集合为空，不能划分
第(2)中情形的当前结点为叶结点，并将其类别设定为该结点所含样本最多的类别，第(3)中情形的当前结点也为叶结点，但将其类别设定为其父结点但所含样本最多的类别。注意这两种情形的处理实质不同：情形(2)是在利用当前结点的后验分布，情形(3)则是把父结点的样本分布作为当前结点的先验分布

# 划分选择
决策树的关键是选择最优划分属性。一般而言，随着划分过程的不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”(purity)越来越高

## 信息增益
“信息熵”(information entropy)是度量样本集合纯度最常用的一种指标。假定当前样本集合D中第k类样本所占的比例为pk(k=1,2,……,|y|)，则D的信息熵定义为

$Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k$

Ent(D)的值越小，则D的纯度越高
假定离散属性a有V个可能的取值${a^1,a^2,a^3,……,a^V}$,若使用a来对样本集D进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为$a^v$的样本，即为$D^v$。考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$|D^v|/|D|$，即样本数越多的分支结点的影响越大，可计算处用属性a对样本集D进行划分所获得的"信息增益"(information gain)

$Gain(D,a)=Ent(D)-\sum_{v=1}^V \frac{|D^v|}{D}Ent(D^v)$

一般而言，信息增益越大，意味着使用属性a来进行划分所得的“纯度提升”越大。
我们可以用信息增益来进行决策树划分属性选择
$a*=arg maxGain(D,a)$，

## 增益率
信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来不利影响。著名的C4.5决策树算法不直接使用信息增益，而是使用“增益率”(gain ratio)来选择最优划分属
性。其定义为

$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$

$IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|} log2\frac{|D^v|}{|D|}$

称为属性a的"固定值"(intrinsic value)。属性a的可能取值数目越多(即V)越大，则IV(a)的值通常会越大。
需要注意的是，增益率准则对可取值数目较少的属性有所偏好，因此，C4.5算法并不是直接选择曾一律最大的候选划分属性，而是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的

## 基尼指数
CART决策树使用"基尼指数"(Gini index)来选择划分属性，数据集D的纯度可用基尼值来度量

$Gini(D)=\sum_{k=1}^{|y|}\sum_{k'!=k}p_kp_{k'}=1-\sum_{k=1}^{|y|}p_k^2$

Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据集D的纯度越高
属性a的基尼指数定义为

$Gini_index(D,a)=\sum_{v=1}^V\frac{|D^v|}{D}Gini(D^v)$

于是，我们在候选属性集合A中，选择哪个使得划分后基尼指数最小的属性作为最优划分属性，即a*=argmin Gini_index(D,a)

## 剪枝处理
剪枝(pruning)是决策树学习算法对付(“过拟合“)的主要手段。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过你和。因此，可通过主动去掉一些分支离开降低过拟合的风险
决策树剪枝的基本策略有”预剪枝(prepruning)“和”后剪枝(post-pruning)“。预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。
留出法可以用来判断决策树泛化性能是否提升

### 前剪枝
在划分之前，所有样例集中在根结点。若不进行划分，该结点将被标记为叶结点，其类别标记为训练样例数最多的类别。预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。另一方面，有些分支的当前划分虽然不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却可能导致泛化性能暂时下降，但在其基础上进行后续划分却有可能导致泛化性暂时下降，但在其基础上进行的后续划分却可能导致性能能显著提高；预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险

### 后剪枝
后剪枝先从训练集生成一颗完整决策树，后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多

## 连续与缺失值

### 连续值处理
连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值来对结点进行划分。此时，连续属性离散化技术可派上用场。最简单的策略是采用二分法(bi-partition)对连续属性进行处理，这正式C4.5决策树算法中采用的机制
给定样本集D和连续属性a,假定a在D上出现了n个不同的取值，将这些值从小到大进行排序，记为${a^1,a^2,……,a^n}$。基于划分点t可将D分为子集$D_t^-$和$D_t^+$，其中$D_t^-$包含哪些属性a取值不大于t的样本，而$D_t^+$则包含那些属性a取值上大于t的样本呢。显然，对相邻的属性取值$a^i与a^{i+1}$来说，t在区间$[a^i,a^{i+1})$中取任意值所产生的划分结果相同。因此，对连续属性a，我们可考察包含n-1个元素的候选划分点集合

$T_a={\frac{a^i+a^{i+1}}{2}|1<=i<=n-1}$

即把区间$[a^i,a^{i+1})$的中位点$\frac{a^i+a^{i+1}}{2}$作为候选划分点。然后，我们可以像离散属性值一样来考察这些划分点，选择最优的划分点进行样本集合的划分

$Gain(D,a)=max Gain(D,a,t)=max Ent(D)-\sum_{\lambda \in{-,+}} \frac{|D_t^\lambda|}{|D|}Ent(D_t^\lambda)$

其中Gain(D,a,t)是样本集D基于划分点t二分后的信息增益。于是，我们就可选择使Gain(D,a,t)最大化的划分点

需注意的是，于离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性

